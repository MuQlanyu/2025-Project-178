# Link Review

-------


| Topic              | Title                                                                                             | Year | Authors             | Paper                                                                                                                   | Code                                  | Summary         |
|:-------------------|:--------------------------------------------------------------------------------------------------|-----:|:--------------------|:------------------------------------------------------------------------------------------------------------------------|:--------------------------------------|:----------------|
| Main article       | Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models                     | 2024 | Zixiang Chen et al. | [arXiv](https://arxiv.org/abs/2401.01335), [OpenReview](https://openreview.net/forum?id=O4cHTxW9BS)                     | [Git](https://github.com/uclaml/SPIN) | TODO            |
| Competitor         | Direct Preference Optimization: Your Language Model is Secretly a Reward Model                    | 2023 | Rafailov et al.     | [NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html) | TODO                                  | TODO            |
|                    | Some things are more cringe than others: Preference opti- mization with the pairwise cringe loss. | 2023 | Jing Xu et al.      | [arXiv](https://arxiv.org/abs/2312.16682)                                                                               | TODO                                  | "Iterative" DPO |
|                    | Self-Rewarding Language Models                                                                    | 2024 | Jing Xu et al.      | [OpenReview](https://openreview.net/forum?id=0NphYCmgua)                                                                                                          | TODO                                  | TODO            |
| Supporting article | LORA: LOW-RANK ADAPTATION OF LARGE LAN- GUAGE MODELS                                              | 2021 | Edward Hu et al.    | [OpenReview](https://openreview.net/forum?id=nZeVKeeFYf9)                                                               | TODO                                  | TODO            |
| Model Report       | Qwen2.5-1M Technical Report                                                                       | 2025 | Yang An et al.      | [arXiv](https://arxiv.org/abs/2501.15383)                                                                               | TODO                                  | TODO            |

------

## Datasets


| Title      | Link                                                                          | Description |
|:-----------|:------------------------------------------------------------------------------|:------------|
| GSM8K      | [HuggingFace](https://huggingface.co/datasets/openai/gsm8k)                   | TODO        |
| Hellaswag  | [HuggingFace](https://huggingface.co/datasets/Rowan/hellaswag)                | TODO        |
| Winogrande | [AllenAI](https://leaderboard.allenai.org/winogrande/submissions/get-started) | TODO        |
| MMLU-PRO   | [Git](https://github.com/TIGER-AI-Lab/MMLU-Pro)                               | TODO        |
| MMLU       | [HuggingFace](https://huggingface.co/datasets/lukaemon/mmlu)                  | TODO        |

