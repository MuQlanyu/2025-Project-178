# Секция с кодом

В этой директории содержится код и скрипты для работы со статьей.

> [IMPORTANT]
> На данный момент код скопирован c небольшими поправками из репозитория [SPIN](https://github.com/uclaml/SPIN/tree/main). В дальнейшем он будет модифицирован под данную статью

## Установка

Клонирование репозитория:
```commandline
git clone git@github.com:intsystems/2025-Project-178.git
cd code
```

## Применение

Предполагается, что модель уже обучена на некотором датасете на этапе SFT

Дальнейшие указания необходимы для дообучения.

### 1. Форматирование датасета

Необходимо привести датасет в формат, специфичный для данного метода, а именно

```commandline
{
    "real": [{"role": "user", "content": <prompt>}, 
               {"role": "assistant", "content": <ground truth>}],
    "generated": [{"role": "user", "content": <prompt>}, 
                 {"role": "assistant", "content": <empty>}]
}
```

Если используется датасет отличный от `ultrachat_200k`, то необходимо самостоятельно привести данные в вышеописанный формат. Для датасета `ultrachat_200k' реализован следующий скрипт перевода в необходимый формат

```commandline
python ./reformat.py [Options]
```

- `output_dir` - директория с результатом (`default: formatted`)

### 2. Генерация ответов 

Далее необходимо заполнить поле `<empty>` в датасете сгенерированными ответами

```commandline
python ./generate.py [Options]
```

- `model` - название модели на HuggingFace или путь к локальной папке
- `tokenizer` - название токенайзера на HuggingFace или путь к локальной папке
- `output_dir` - директория с результатом (`default: generated/iter0`)  
- `batch_size` - размер батча для генерации 
- `input_dir` - директория, содержащая датасет в формате из пункта 1 (`default: formatted`)

Поддерживается опция генерации ответов по частям, за это отвечают следующие параметры

- `data_frac` - порядок части датасета
- `frac_len` - размер части датасета. Если `frac_len = 0`, то генерация происходит по всем данным, иначе только по части `data_frac` (`default: 0`)

Поддерживается опция генерации, используя веса только адаптеров Lora. Это может быть полезно для уменьшения занимаемой памяти моделями и чекпоинтами, так как адаптеры занимают намного меньше памяти

В случае использования данной опции параметр `model` отвечает за путь к чекпоинту обучения модели с адаптерами, то есть к весам адаптеров

- `base_model` - название базовой модели на HuggingFace или путь к локальной папке с ее весами
- `lora_r` - параметр `r` в конфиге `peft.LoraConfig`, использованный при обучении модели. Если данный `lora_r = 0`, то данная опция не используется (`default: 0`)
- `lora_alpha` - параметр `alpha` в конфиге `peft.LoraConfig`, использованный при обучении модели

После генерации ответов необходимо собрать все результаты в датасет (даже, если генерация проходила по всему `input_dir` датасету).

```commandline
python ./convert_data [Options]
```

- `num_fracs` - количество частей при генерации ответов
- `input_dir` - директория с сгенерированными ответами (`default: generated/iter0`)
- `output_dir` - директория с результатами (`default: datasets/iter0`)

### 3. Дообучения модели

```commandline
python ./run_spin.py [config]
```

В объекте `[config]` содержатся параметры обучения, информация о названии модели или пути к чекпоинту, названии токенайзера, использовании Lora и датасете. 

Примеры `[config]` можно увидеть в `../experiment/*/configs/*.yaml`

Подробное описание каждого параметра находится в `./lib/configs`


## Возможные проблемы

- На этапе 3, может потребоваться понижении версии `numpy` до `1.26.4`

- На данный момент 3-ий этап не поддерживает опцию использования весов только адаптеров, поэтому в качестве модели необходимо подавать путь к весам всей модели целиком.