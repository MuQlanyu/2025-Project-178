\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\usepackage[]{algorithmic}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{caption}
\usepackage{subfig}

\usepackage[english, russian]{babel}

%images
\usepackage{graphicx}
\graphicspath{./images/} 

%hyperlinks
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }


% Compatibility for biblatex
\usepackage{csquotes}

% Load biblatex before cleveref
\usepackage[backend=bibtex]{biblatex}
\addbibresource{references.bib}


\newcommand{\hdir}{.}
\newtheorem{statement}{Утверждение}

\newcommand{\bz}{\mathbf{z}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\bOmega}{\boldsymbol{\Omega}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\bDelta}{\boldsymbol{\Delta}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\bchi}{\boldsymbol{\chi}}
\newcommand{\bzeta}{\boldsymbol{\zeta}}
\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\beps}{\boldsymbol{\varepsilon}}
\newcommand{\bZeta}{\boldsymbol{Z}}
% mathcal
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cF}{\mathcal{F}}

\newcommand{\HH}{\mathbb{H}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\EE}{\mathbb{E}}
% transpose
\newcommand{\T}{^{\mathsf{T}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}

% method name
% \newcommand{\mname}{LoSPIN}



\begin{document}

\title
    [] % краткое название; не нужно, если полное название влезает в~колонтитул
    {Low-rank self-play fine-tuning for small LLMs}
\author
    [] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
    {П.\,Ю.~Мун, Н.\,В.~Охотников, А.\,В.~Грабовой} % основной список авторов, выводимый в оглавление
    [П.\,Ю.~Мун$^1$, Н.\,В.~Охотников$^2$, А.\,В.~Грабовой$^3$] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
%\email
%   {anton39reg@mail.ru; anton.khritankov@phystech.edu}

%\thanks
%    {Работа выполнена при
%     %частичной
%     финансовой поддержке РФФИ, проекты \No\ \No 00-00-00000 и 00-00-00001.}
%\organization
%    {$^1$Организация, адрес; $^2$Организация, адрес}
\abstract
  {В работе исследуется проблема дообучения больших языковых моделей (LLM) в условиях ограниченных ресурсов. Под ограниченными ресурсами понимается видеопамять, человеческое участие и время обучения. В работе рассматриваются модели до 0.5B. Предлагается метод дообучения, основанный на внедрении адаптеров LoRA, малоранговых разложений матриц, в слои архитектуры трансформера, и использовании стратегии self-play - текущая итерация генерирует предсказания, а обучающаяся повышает качество с помощью разграничения настоящих предсказаний от сгенерированных. Метод может снизить требования по видеопамяти для обучения на доступных GPU: Google Colab T4 (16Gb), а также не требует размеченных данных помимо используемых на этапе SFT. Для анализа качества метода будет использована группа датасетов таких, как gsm8k, ultrachat_200k.


\bigskip
\noindent
\textbf{Ключевые слова}: \emph {LLM, LoRA, selp-play, SFT}
}
%данные поля заполняются редакцией журнала
\doi{}
\receivedRus{}
\receivedEng{}

\maketitle
\linenumbers
\section{Введение}
Большие языковые модели (LLM) демонстрируют исключительные возможности в широком спектре областей, которые могут требовать специализированных знаний. Примерами таких областей могут служить: программирование \parencite{Chen2021}, генерация текстов \parencite{Touvron2023}, обращения к базам данных \parencite{Zhong2017}. Обычно процесс обучения LLM состоит из нескольких этапов: предварительного обучения, и этапов дообучения: SFT, обучение на предварительно размеченном наборе данных, и RLHF, во время которого необходим эксперт-человек, оценивающий ответы модели. Предварительное обучение требует огромных вычислительных ресурсов, поэтому часто используют публичные предобученные модели (Llama3 \parencite{Dubey2024}, Qwen2.5 \parencite{Yang2025}) и настраивают под целевую задачу. Но также проблема ограниченности ресурсов встречается и на этапах дообучения SFT и RLHF. Ограниченность ресурсов проявляется в недостатке видеопамяти для хранения и обновления параметров модели, необходимости в размеченных данных для повышения качества и времени обучения.

В работе исследуются методы повышения эффективности обучения моделей в условиях ограниченных ресурсов. В частности предлагается метод дообучения LLM, который значительно снижает потребление видеопамяти, а также убирает необходимость в прямом человеческом участии, как на этапе RLHF. Метод основан на двух идеях. 

\begin{itemize}
  \item Во-первых, внедрение адаптеров LoRA в слои трансформера. Предполагается сравнительно маленькая внутренняя размерность пространства параметров, и рассматриваемый метод снижает размерность пространства параметров. 
  
  \vspace{8}
   
  \item Во-вторых, механизме self-play для обучения адаптеров. Механизм состоит из последовательных игр модели со своей предыдущей версией. Предыдущая версия генерирует ответы по промптам части датасета на этапе SFT, а модель пытается различить настоящий ответ от сгенерированного. Общий метод исследуется в статье \parencite{Chen2024a}, а в данной работе он применяется исключительно к адаптерам LoRA. 
\end{itemize}

Предложенный метод развивается в двух направлениях: снижение требований к видео памяти и снижение человеческого участия. В обоих направлениях есть близкие альтернативы:

\begin{itemize}
  \item Многие существующие решения по снижению требуемой памяти также применяют адаптеры LoRA с использованием других инструментов. Одним из лучших методов является QLoRA \parencite{Dettmers2023}, который предлагает 4-х битные float и квантизацию, значительно снижая необходимую память. В отличие от LoRA, которая снижает количество обучаемых параметров, QLoRA также уменьшает размер параметров модели с помощью 4-х битных NormalFloat, что приводит в меньшим требованиям к видеопамяти. С другой стороны, QLoRA требует передового оборудования для применения, что может являться существенным ограничением в условиях нехватки ресурсов. 

  \vspace{8}
  
  \item В направлении снижения человеческого участия можно выделить методы оптимизирующие этап RLHF. Одним из таких является метод DPO \parencite{Rafailov2023a}, который снижает зависимость от человеческого оценивания результата. В то же время предложенный метод не требует размеченных данных, помимо тех, что используются на SFT.
\end{itemize}

Конечной целью работы является исследование оправданности применения предложенного метода к маленьким LLM в условиях ограниченных ресурсов.

\vspace{10}

\section{Постановка задачи}

Будем обозначать за $\bTheta$ - пространство всевозможных параметров трансформера, $p_{\btheta}$ - модель , а $\btheta \in \bTheta$ -  ее параметры. В задаче ставится ограничение на количество параметров модели, то есть $\|\bTheta\| \le \mathbf{K}$. В качестве аргумента функция $p_{\btheta}$ принимает последовательность $\bx = [x_1, ..., x_n]$ и возвращает последовательность $\by=[y_1, ..., y_m]$, которые интерпретируются как промпт, ответ модели. Модель $p_{\btheta}$ представима в виде условной плотности, которую можно представить как:

$$p_{\btheta}(\by|\bx) = \prod_{j=1}^m p_{\btheta}(y_j|\bx, y_{<j})$$

где $y_{<j} = [y_1, ..., y_{j-1}$, для $j \in [2, ..., m]$, а $p_{\btheta}(y_1|x, y_{<1}) = p_{\btheta}(y_1|x)$.

На этапе SFT ставится задача минимизации следующей лосс-функции:

\begin{align}
    L_{SFT}(\btheta) = -\EE_{\bx \sim q(\cdot), \by \sim p_{data}(\cdot|\bx)}[log p_{\btheta}(\by|\bx)]
\end{align}

где $\bx$ - случайный элемент из распределения промптов $q(\cdot)$, $\by$ - случайный элемент из распределения ответов $p_{data}(\cdot |\bx)$

\vspace{10}

\section{Метод}


В данной секции будет рассмотрен предложенный метод, позволяющий в условиях ограниченных ресурсов повысить метрики в сравнении с метриками на этапе SFT. Процесс дообучения начинается с определения базовой модели $p_{\btheta_0}$, где $\btheta_0 \in \bTheta$ - параметры базовой модели. Далее, на этапе SFT обучение проходит на размеченном тренировочном датасете $\{(\bx, \by)\}_{i=1}^n$ из распределения $q(\cdot)$ и $p_{data}(\cdot | \bx)$. Предложенный метод является итеративным, поэтому в дальнейшем будем обозначать за $\btheta_t$ - параметры обучаемой модели на итерации $t$.

Теперь перейдем к введению понятий для обозначения методов.

\paragraph{Адаптеры LoRA}
Адаптер LoRA представлен произведением двух матриц $A \cdot B$ и может быть встроен в любую обучаемую матрицу весов $W_0$. Во время дообучения матрица весов $W_0$ заморожена, то есть не меняется, и обучаются только матрицы $A$ и $B$, что может помочь снизить количество обучаемых параметров и соответственно требуемую память для дообучения. В конце модель использует сумму изначальной матрицы с адаптером, $W = W_0 + A \cdot B$.

В данной работе адаптеры LoRA встраиваются на всех слоях в матрицы: $W_q, W_k, W_v$ - query/key/value матрицы блоков self-attention в архитектуре трансформера. Обозначим значения всех встраиваемых параметров на шаге t за $\bDelta\btheta_t$, тогда

$$\btheta_t = \btheta_0 + \bDelta\btheta_t$$

\paragraph{Подробное описание SPIN}

В центре механизма self-play находится игра между: моделью игроком и моделью противником. Цель противника сгенерировать ответы, которые были бы неразличимы с настоящими ответами, а цель игрока уметь различать настоящие ответы от сгенерированных. В рамках дообучения противником является прошлая итерация модели, а игроком текущая итерация, которая обучается.

Рассмотрим итерацию t+1, противником на данной итерации является $p_{\btheta_t}$, которая по промптам $\bx$ генерирует ответы $\by'$, а игроком $p_{\btheta_{t+1}}$. Метод состоит из двух шагов: обучение игрока, обновление противника

\begin{itemize}
    \item В качестве целевой функции для первого шага используется:
    \begin{align}
        f_{t+1} &= \argmin_{f \in \cF_{t}}\EE_{\bx\sim q(\cdot), \by\sim p_{data}(\cdot | \bx)}\big[l(f(\bx, \by) - f(\bx, \by')) \big], \label{eq:f*1}
    \end{align}
    , где $l = log(1 + exp(-t))$, а $\cF_t$ - некоторое семейство функций.

    \vspace{8}

    \item Для обновления противника используется следующая функция:
    
    \begin{align} 
        p_{\btheta_{t+1}} = \argmax_ {p}\EE_{\bx \sim q(\cdot), \by\sim p(\cdot|\bx)} [f_{t+1}(\bx, \by)] - \lambda \EE_{\bx\sim q(\cdot)}\mathrm{KL}\big(p(\cdot|\bx)||p_{\btheta_t}(\cdot|\bx)\big), \phantomsection\label{eq:update}
    \end{align} 
    
    , где $\lambda$ - коэффициент регуляризации похожести итераций, а $KL$ - дивергенция Кульбака-Лейблера. Регуляризация добавлена с целью контроля обучаемой итерации, чтобы $p_\btheta_{t+1}$ не сильно отличалась от $p_\btheta_t$
    
    
\end{itemize}

\paragraph{Одношаговое описание SPIN}

Примечательно, что последнее выражение \eqref{eq:update} на $p_{\btheta_{t+1}}$ имеет аналитическое решение $\hat p_{\btheta_{t+1}}$:

\begin{align}
\hat{p}_{\btheta_{t+1}}(\by|\bx) \propto p_{\btheta_t}(\by|\bx) \exp\big(\lambda^{-1}f_{t+1}(\bx, \by)\big). \label{eq:closed form solution}  
\end{align}

Важно упомянуть, что у такого аналитического решения может не быть представления в виде функции, параметризованной элементами из $\bTheta$, то есть $\hat{p}_{\btheta_{t+1}}$ не обязательно принадлежит семейству $\{p_{\btheta} | \btheta \in \bTheta\}$. Тогда рассмотрим класс функций для $f_{t+1}$:

\begin{align}
\cF_{t} = \bigg\{\lambda\cdot \log \frac{p_{\btheta}(\by | \bx)}{p_{\mathrm{\btheta_t}}(\by | \bx)}\bigg|\btheta \in \bTheta\bigg\},   \label{eq:function class0} 
\end{align}

Подставляя элементы полученного семейства в формулу 1, получим искомый критерий качества, используемый при обучении.

\begin{align}
L_{SPIN}= \EE_{\bx\sim q(\cdot), \by\sim p_{data}(\cdot | \bx)}\bigg[\ell\bigg(\lambda \log \frac{p_{\btheta}(\by | \bx)}{p_{\btheta_t}(\by | \bx)}-\lambda \log \frac{p_{\btheta}(\by' | \bx)}{p_{\btheta_t}(\by' | \bx)}\bigg)\bigg], \label{eq:loss}  
\end{align}

Тогда правило обновление параметров имеет вид:

\begin{align}
    \btheta_{t+1} = \argmin_{\btheta \in \bTheta} L_{SPIN} (\btheta, \btheta_t)
\end{align}


\paragraph{Применение SPIN к адаптерам}

Обозначим за $\bOmega \subset \bTheta$ - подпространство весов адаптеров LoRA в архитектуре трансформера. Заметим, что использование параметров адаптеров LoRA $\bDelta\btheta$ сужает рассматриваемый класс функций $\{p_{\btheta_0 + \bDelta\btheta} | \bDelta\btheta \in \bOmega\} \subset \{p_{\btheta} | \btheta \in \bTheta\}$, или же снижает размерность пространства обучаемых параметров, но все равно может быть применим в полученному критерию качества $L_{SPIN}$

\begin{align}
    \bDelta\btheta_{t+1} = \argmin_{\bDelta\btheta \in \bOmega} L_{SPIN} (\btheta_0 + \bDelta\btheta, \btheta_0 + \bDelta \btheta_t)
\end{align}

\begin{algorithm}[ht!]
\caption{Self-Play Fine-Tuning с адаптерами LoRA}\label{alg:Improving}
\begin{algorithmic}%[l]
\STATE \textbf{Дано:} $\{(\bx_i, \by_i)\}_{i\in \{1 ... n\}}$: SFT датасет, $T$: Количество итераций. 
\FOR{$t = 0,\ldots, T-1$}
\FOR{$i = 1, \ldots N$}
\STATE Генерация ответов моделью противником: $\by_{i}' \sim p_{\btheta_0 + \bDelta \btheta_t}(\cdot|\bx_i)$.
\ENDFOR 
\STATE  $\bDelta\btheta_{t+1} = \argmin_{\bDelta\btheta \in \bOmega} \sum_{i = 1}^n\ell\Big(\lambda \log \frac{p_{\btheta + \bDelta \btheta}(\by_i | \bx_i)}{p_{\btheta_0 + \bDelta \btheta_t}(\by_i | \bx_i)}-\lambda \log \frac{p_{\btheta + \bDelta \btheta}(\by'_i | \bx_i)}{p_{\btheta_0 + \bDelta \btheta_t}(\by'_i | \bx_i)}\Big)$.
\ENDFOR
\STATE \textbf{Вывод:} $\bDelta \btheta_T$.
\end{algorithmic}
\end{algorithm}

\vspace{10}

\section{Вычислительный эксперимент}

Целью данного эксперимента является подтверждение сохранения или повышению качества модели при дообучении предложенным методом, по сравнению с обычным применением метода SPIN. Особое внимание уделяется контролю затрачиваемых ресурсов и измерению времени обучения.

Для начало необходимо задать ограничение $\mathbf{K} = 5\cdot10^8$, отвечающий за ограничение на количество параметров модели $\| \bTheta\| \le \mathbf{K}$. Значение обусловлено используемым оборудованием Tesla T4 из Google Colab, память 16 GB, количество ядер 2560. 

\paragraph{Описание эксперимента}
В качестве предобученной модели взята трансформерная модель \href{https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct}{qwen2.5-0.5B-Instruct} для задачи генерации текстов, так как она является sota-решением среди моделей с небольшим количеством параметров и достигает верхней границы на параметры.

% Обучение происходит на датасете \href{https://huggingface.co/datasets/openai/gsm8k}{GSM8K}, состоящем из более чем 8000 математических задач на простую арифметику: $+, -, \%, \cdot$. Все задачи данного уровня должен быть способен решать ученик средних классов. Ответ представлен в виде рассуждения, использующего математические вычисления.
Обучение происходит на датасете \href{https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k}{ultrachat\_200k}, состоящем из более чем 200000 элементов вида вопрос-ответ. Во время обучения использовалась лишь небольшая часть данных, а именно 2000 элементов, в силу ограничения по времени.

Рассмотрены три вариации архитектур, зависящие от наличия и размера адаптеров LoRA: две модели с адаптерами (lora\_r = 8, lora\_r = 16) и одна модель без адаптеров (without lora). Обучения моделей с адаптерами происходило на заявленных вычислительных ресурсах, а именно Google Colab T4, а модель без адаптеров обучалась на видеокарте A100, так как для ее обучения требуется больше видеопамяти. 

Каждая модель обучалась 3 эпохи на этапе SFT, а также по 3 эпохи на двух итерациях SPIN.

В качестве метрики использовалась метрика BLEU

С результатами эксперимента можно ознакомиться в \href{https://github.com/intsystems/2025-Project-178}{репозитории}

\paragraph{Результаты}

\vspace{10}

% \begin{table}[]
\begin{tabular}{l|l|l|l|}
model                  & trainable params & BLEU (SFT)       & BLEU (LoRA + SPIN) \\ \hline
qwen2.5 (lora\_r = 8)  & 1M               & 0.06454 (93.4\%) & 0.06554 (93.2\%)   \\ \hline
qwen2.5 (lora\_r = 16) & 2.2M             & 0.06420 (92.9\%) & 0.06895 (98.0\%)   \\ \hline
qwen2.5 (without lora) & 494M             & 0.06912 (100\%)  & 0.07035 (100\%)    \\ \hline
\end{tabular}
% \end{table}

\vspace{10}

Видно, что после этапа SFT модели с адаптерами проигрывают модели без адаптеров примерно на 6.6\% и 7.1\%, но после дообучения предложенным методом вариация с lora\_r = 16 проигрывает модели без адаптеров только на 2\%.

Говоря о времени обучения и пиковых значений нагрузки видеокарты, то модели с адаптерами обучались 8 часов 40 минут, а модель без адаптеров 2.5 часа. В это время входит время обучения на этапе SFT, время генерации датасета для применения SPIN моделью-противником, то есть предыдущей итерацией модели, а также применения непосредственного обучения модели-игрока.

В данном эксперименте внедрение адаптеров LoRA снижает количество обучаемых параметров примерно в 500 и 250 раз соответственно. Но при этом разница между пиковой нагрузкой на GPU несущественна, а именно максимальное значение используемой видеопамяти достигало 15.2 GB для обеих моделей с адаптерами. С другой стороны, обучение модели без адаптеров достигало 38GB с увеличенным размером batch\_size на A100.

\section{Заключение и выводы}

В работе был предложен метод, являющийся комбинацией двух уже существующих методов, LoRA и SPIN, которые направлены на дообучение моделей в условиях ограниченных ресурсов. Внедрение адаптеров LoRA снизило необходимое количество видеопамяти примерно в 2 раза, что позволило обучать модель Qwen2.5-0.5B-Instruct на доступных вычислительных ресурсах, Google Colab T4, при этом качество модели по метрике BLEU падает лишь на 2\% по сравнению с обычным применением метода SPIN.


\printbibliography

\end{document}

