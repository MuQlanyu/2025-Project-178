\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\usepackage[]{algorithmic}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{caption}
\usepackage{subfig}

\usepackage[russian]{babel}


% Compatibility for biblatex
\usepackage{csquotes}

% Load biblatex before cleveref
\usepackage[backend=bibtex]{biblatex}
\addbibresource{references.bib}


\newcommand{\hdir}{.}
\newtheorem{statement}{Утверждение}

\newcommand{\bz}{\mathbf{z}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\bOmega}{\boldsymbol{\Omega}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\bDelta}{\boldsymbol{\Delta}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\bchi}{\boldsymbol{\chi}}
\newcommand{\bzeta}{\boldsymbol{\zeta}}
\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\beps}{\boldsymbol{\varepsilon}}
\newcommand{\bZeta}{\boldsymbol{Z}}
% mathcal
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cF}{\mathcal{F}}

\newcommand{\HH}{\mathbb{H}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\EE}{\mathbb{E}}
% transpose
\newcommand{\T}{^{\mathsf{T}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}

% method name
% \newcommand{\mname}{LoSPIN}



\begin{document}

\title
    [] % краткое название; не нужно, если полное название влезает в~колонтитул
    {Low-rank self-play fine-tuning for small LLMs}
\author
    [] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
    {П.\,Ю.~Мун, Н.\,В.~Охотников, А.\,В.~Грабовой} % основной список авторов, выводимый в оглавление
    [П.\,Ю.~Мун$^1$, Н.\,В.~Охотников$^2$, А.\,В.~Грабовой$^3$] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
%\email
%   {anton39reg@mail.ru; anton.khritankov@phystech.edu}

%\thanks
%    {Работа выполнена при
%     %частичной
%     финансовой поддержке РФФИ, проекты \No\ \No 00-00-00000 и 00-00-00001.}
%\organization
%    {$^1$Организация, адрес; $^2$Организация, адрес}
\abstract
  {В работе исследуется проблема дообучения больших языковых моделей (LLM) в условиях ограниченных ресурсов. Под ограниченными ресурсами понимается видеопамять, человеческое участие и время обучения. В работе рассматриваются модели до 1.5B. Предлагается метод дообучения, основанный на внедрении адаптеров LoRA, малоранговых раложений матриц, в слои архитектуры трансформера, и использовании стратегии self-play - текущая итерация генерирует предсказания, а обучающаяся повышает качество с помощью разграничения настоящих предсказаний от сгенерированных. Метод может снизить количество обучаемых параметров в 10000, и память в три раза, также он не требует размеченных данных помимо используемых на этапе SFT. Для анализа качества метода будет использована группа датасетов, таких как MMLU, Winogrande.


\bigskip
\noindent
\textbf{Ключевые слова}: \emph {LLM, LoRA, selp-play, SFT}
}
%данные поля заполняются редакцией журнала
\doi{}
\receivedRus{}
\receivedEng{}

\maketitle
\linenumbers
\section{Введение}
Большие языковые модели (LLM) демонстрируют исключительные возможности в широком спектре областей, которые могут требовать специализированных знаний. Примерами таких областей могут служить: программирование \parencite{Chen2021}, генерация текстов \parencite{Touvron2023}, обращения к базам данных \parencite{Zhong2017}. Обычно процесс обучения LLM состоит из нескольких этапов: предварительного обучения, контролируемой тонкой натройки(SFT) и обучения с подкреплением на основе отзывов людей (RLHF). Предварительное обучение требует огромных вычислительных ресурсов, поэтому часто используют публичные предобученные модели (Llama3 \parencite{Dubey2024}, Qwen2.5 \parencite{Yang2025}) и дообучают под целевую задачу. Часто проблема жесткой ограниченности ресурсов встречается при дообучении маленьких LLM. Ограниченность ресурсов проявляется в недостатке видеопамяти для хранения и обновления параметров модели, необходимости в размеченных данных для повышения качества и времени обучения.

По вышеперечисленным причинам хотелось бы исследовать методы, которые бы использовали меньше ресурсов и не понижали качество модели. В данной работе предлагается метод дообучения LLM, который значительно снижает потребление видеопамяти, а также убирает необходимость в прямом человеческом участии без снижения качества модели. Метод основан на двух идеях. 

\begin{itemize}
  \item Во-первых, внедрение адаптеров LoRA в слои трансформера. Предполагается сравнительно маленькая внутренняя размерность обучаемых матриц модели, и вместо обучения всей матрицы добавляют приращение в виде малорангового разложения в две обучаемые матрицы, замораживая параметры изначальной. 
  
  \vspace{8}
   
  \item Во-вторых, механизме самостоятельной игры (self-play) для обучения адаптеров. Общий метод исследуется в статье \parencite{Chen2024a}, а  в данной работе он применяется исключительно к адаптерам LoRA. Механизм состоит из последовательных игр модели со своими прошлыми итерациями. Прошлая итерация генерирует ответы по промптам части датасета на этапе SFT, а модель пытается различить настоящий ответ от сгенерированного. Предполагается наличие распределения у настоящих ответов датасета и полученная модель лучше настроена под предполагаемое распределение.
\end{itemize}

Предложенный метод развивается в двух направлениях: снижение требований к видео памяти и снижение человеческого участия. В обоих направлениях есть близкие альтернативы:

\begin{itemize}
  \item Многие существующие решения по снижению требуемой памяти также применяют адаптеры LoRA с использованием других интсрументов. Одним из лучших методов является QLoRA \parencite{Dettmers2023}, который применяет 4х битные NormalFloat и квантизацию, значительно снижая необходимую память. В отличии от LoRA, которая снижает количество обучаемых параметров, QLoRA также уменьшает размер параметров модели с помощью 4х битных NormalFloat, что приводит в меньшим требованиям к видеопамяти. С другой стороны, QLoRA требует передового оборудования для применения, что редко доступно в условиях ограниченных ресурсов. 

  \vspace{8}
  
  \item В направлении снижения человеческого участия можно выделить методы оптимизирующие этап RLHF. Одним из таких являетя метод DPO \parencite{Rafailov2023a}, который снижает зависимость от человеского оценивания результата. В то же время предложенный метод не требует размеченных данных, помимо тех, что используются на SFT.
\end{itemize}

Конечной целью работы является исследование оправданности применения предложенного метода к маленьким LLM в условиях ограниченных ресурсов.

[Тут будет сравнительная таблица, и по мере исследования она будет наполняться.]

\vspace{10}

\section{Связанные работы}

TODO:

\vspace{10}

\section{Постановка задачи}

Будем обозначать за $\bTheta$ - пространство всевозможных параметров трансформера, $p_{\btheta}$ - модель , а $\btheta \in \bTheta$ -  ее параметры. На вход модель принимает промпт $\bx = [x_1, ..., x_n]$ и возвращает ответ $\by' = [y_1', ..., y_m']$, который должен согласовываться с истинным ответом $\by=[y_1, ..., y_m]$.

Так как рассматривается дообучение модели, то у модели есть начальные параметры $\btheta_0$ и параметры итераций дообуения $\btheta_t$

Также для дальнейшей работы с методом введем вероятностные предположения о распределениях данных: $\bx \sim q(\cdot)$,  $\by \sim p_{data}(\cdot | \bx)$. 

Теперь перейдем к введению понятий для обозначений методов.

\paragraph{Адаптеры LoRA}
Адаптеры LoRA встраиваются на всех слоях в матрицы: $W_q, W_k, W_v$ - query/key/value матрицы блоков self-attention в архитектуре трансформера. Тогда итоговая матрица параметров имеет вид: $W = W_0 + A * B$, где $W_0$ - изначальная матрица замороженных весов, а $A * B$ - адаптер.

Обобщая с матриц на все параметры модели, получим формулу 

$$\btheta_t = \btheta_0 + \bDelta\btheta_t$$

, где $\bDelta \btheta_t$ - веса адаптеров LoRA на всех слоях, а $\btheta_0$ - замороженные веса исходной модели

\paragraph{Подробное описание SPIN}

В центре механизма self-play находится игра между: игроком и противником. Цель противника сгенерировать ответы, которые были бы неразличимы с настоящими ответами, а цель игрока уметь различать настоящие ответы от сгенерированных. В рамках дообучения опонентом является прошлая итерация модели, а игроком текущая итерация, которая обучается.

Рассмотрим итерацию t+1, противником на данной итерации является $p_{\btheta_t}$, которая по промптам $\bx$ генерирует ответы $\by'$, а игроком $p_{\btheta_{t+1}}$. Метод состоит из двух шагов: обучение игрока, обновление опонента

\begin{itemize}
    \item В качестве целевой функции для первого шага используется:
    \begin{align}
        f_{t+1} &= \argmax_{f \in \cF_{t}}\EE_{\bx\sim q(\cdot), \by\sim p_{data}(\cdot | \bx)}\big[l(f(\bx, \by) - f(\bx, \by')) \big], \label{eq:f*1}
    \end{align}
    , где $l = log(1 + exp(-t))$, семейство функций $\cF_t$ будет явно задано позже

    \vspace{8}

    \item Для обновления опонента используется следующая функция:
    \begin{align}
        p_{\btheta_{t+1}} = \argmax_ {p}\EE_{\bx \sim q(\cdot), \by\sim p(\cdot|\bx)} [f_{t+1}(\bx, \by)]\notag - \lambda \EE_{\bx\sim q(\cdot)}\mathrm{KL}\big(p(\cdot|\bx)||p_{\btheta_t}(\cdot|\bx)\big), \label{eq: update}
    \end{align} 
    , где $\lambda$ - коэффициент регуляризации похожести итераций, а $KL$ - дивергенция Кульбака-Лейблера. Регуляризация добавлена с целью контроля обучаемой итерации, чтобы $p_\btheta_{t+1}$  не сильно отличалась от $p_\btheta_t$
    
    
\end{itemize}

\paragraph{Одношаговое описание SPIN}

Из последней формулы можно вывести оптимальное значение 

\begin{align}
\hat{p}(\by|\bx) \propto p_{\btheta_t}(\by|\bx) \exp\big(\lambda^{-1}f_{t+1}(\bx, \by)\big). \label{eq:closed form solution}  
\end{align}

Данная модель может не быть в рассматриваемом семействе моделей $\{p_{\btheta} | \btheta \in \bTheta\}$, но отсуда можно вывести семейство моделей для $f_{t+1}$:

\begin{align}
\cF_{t} = \bigg\{\lambda\cdot \log \frac{p_{\btheta}(\by | \bx)}{p_{\mathrm{\btheta_t}}(\by | \bx)}\bigg|\btheta \in \bTheta\bigg\},   \label{eq:function class0} 
\end{align}

Подставляя элементы полученного семейства в формулу 1, получим искомый критерий качеста, используемый при обучении.

\begin{align}
L_{SPIN}= \EE_{\bx\sim q(\cdot), \by\sim p_{data}(\cdot | \bx)}\bigg[\ell\bigg(\lambda \log \frac{p_{\btheta}(\by | \bx)}{p_{\btheta_t}(\by | \bx)}-\lambda \log \frac{p_{\btheta}(\by' | \bx)}{p_{\btheta_t}(\by' | \bx)}\bigg)\bigg], \label{eq:loss}  
\end{align}

Тогда правило обновление параметров выглядит так:

\begin{align}
    \btheta_{t+1} = \argmin_{\btheta \in \bTheta} L_{SPIN} (\btheta, \btheta_t)
\end{align}


\paragraph{Применение SPIN к адаптерам}

Обозначим за $\bOmega \subset \bTheta$ - подпространство весов адаптеров LoRA в архитектуре трансформера.

\begin{align}
    \bDelta\btheta_{t+1} = \argmin_{\bDelta\btheta \in \bOmega} L_{SPIN} (\btheta_0 + \bDelta\btheta, \btheta_0 + \bDelta \btheta_t)
\end{align}

\printbibliography

\end{document}

