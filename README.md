# Low-rank self-play fine-tuning for small LLMs

----

__Expert:__ Andrey Grabovoy

__Consultant:__ Nikita Okhotnikov

----

### Abstract

> Так как, пока нет .tex файла, то аннотация будет пока тут. Также, если будет нужно она будет переведена на английский

__Аннотация:__ Работа исследует проблему сложности дообучения языковых моделей (LLM). Под сложностью подразумевается количество затрачиваемых человеческих ресурсов. Обычно для повышения качества полученной модели используют размеченные данные, что влечет использование большого числа ресурсов. Рассматривается метод SPIN, метод повышения качества дообучения LLM с использованием стратегии self-play, который не требует наличия данных помимо, используемых на дообучении. Предлагается оценить границы применимости данного метода для малых моделей и оправдонности применения метода в условиях ограниченности ресурсов. Для анализа качества метода будет использована группа датасетов, таких как MMLU, Winogrande.